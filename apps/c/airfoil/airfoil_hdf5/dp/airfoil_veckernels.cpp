//
// auto-generated by op2.py on 2015-06-19 13:42
//

// header
#include "op_lib_cpp.h"
#ifdef VECTORIZE
#define SIMD_VEC 4
#define double_ALIGN 128
#define float_ALIGN 64
#define int_ALIGN 64
#define ALIGNED_double __attribute__((aligned(double_ALIGN)))
#define ALIGNED_float __attribute__((aligned(float_ALIGN)))
#define ALIGNED_int __attribute__((aligned(int_ALIGN)))
#else
#define ALIGNED_double
#define ALIGNED_float
#define ALIGNED_int
#endif

#include <immintrin.h>
#include <cstdio>
#include <cstdlib>
#include <cassert>
#include <ctime>

// For double precision on AVX, VLEN = 4
#define VLEN 4

// Gather two adjacent 64-bit quantities and transpose them into SoA form.
inline void gather_transpose_2x64(const double* restrict data,
                                  const int idx[VLEN], double v[2][VLEN]){
  // One 128-bit load for each of our 4 vector lanes.
  // Note the interleaving -- because of the shuffle pattern I use,
  // I store [0, 2] [1, 3] instead of [0, 1] [2, 3].
  __m256d in02 = _mm256_castpd128_pd256(_mm_load_pd(&data[idx[0]]));
  __m256d in13 = _mm256_castpd128_pd256(_mm_load_pd(&data[idx[1]]));
  in02 = _mm256_insertf128_pd(in02, _mm_load_pd(&data[idx[2]]), 1);
  in13 = _mm256_insertf128_pd(in13, _mm_load_pd(&data[idx[3]]), 1);

  // Shuffle sequence to transpose.  This will be the same for other instruction sets.
  // Note that the shuffle is identical for each 128-bits.
  __m256d out0 = _mm256_shuffle_pd(in02, in13, 0b0000);
  __m256d out1 = _mm256_shuffle_pd(in02, in13, 0b1111);

  // One vector-length store per struct element.
  _mm256_store_pd(v[0], out0);
  _mm256_store_pd(v[1], out1);
}

// global constants
extern double gam;
extern double gm1;
extern double cfl;
extern double eps;
extern double mach;
extern double alpha;
extern double qinf[4];
// user kernel files
#include "save_soln_veckernel.cpp"
#include "adt_calc_veckernel.cpp"
#include "res_calc_veckernel.cpp"
#include "bres_calc_veckernel.cpp"
#include "update_veckernel.cpp"